
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Modelo Original &#8212; Modelos de Serie de Timepo para Pronóstico de la Velocidad del Viento</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'OM';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Redes Neuronales Artificiales" href="ANNs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="EDA.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Modelos de Serie de Timepo para Pronóstico de la Velocidad del Viento - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Modelos de Serie de Timepo para Pronóstico de la Velocidad del Viento - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="EDA.html">
                    Análisis Exploratorío de Datos - Velocidad del Viento
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="SE.html">Modelos de Suavización Exponencial</a></li>
<li class="toctree-l1"><a class="reference internal" href="ARIMA.html">Modelos autorregresivos integrados de media móvil</a></li>
<li class="toctree-l1"><a class="reference internal" href="ANNs.html">Redes Neuronales Artificiales</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">Modelo Original</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FOM.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/OM.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Modelo Original</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="modelo-original">
<h1>Modelo Original<a class="headerlink" href="#modelo-original" title="Link to this heading">#</a></h1>
<p>Para la construcción del modelo original se plantea un emsable del modelo ARMA y el modelo TL smoother. La ecuación para la construcción del modelo se define así:</p>
<div class="math notranslate nohighlight">
\[\alpha_t \cdot P_{ARMA_t} + (1 - \alpha_t) \cdot P_{TLsm_t}\]</div>
<p>Donde <span class="math notranslate nohighlight">\(\alpha_t\)</span> es un factor de ponteración para el aporte de las prediciones por obvservación de cada modelo,<br />
<span class="math notranslate nohighlight">\( P_{ARMA_t}\)</span> las predicciones del modelo ARMA y<br />
<span class="math notranslate nohighlight">\(P_{TLsm_t}\)</span> las predicciones del modelo TL smoother.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.diagnostic</span> <span class="kn">import</span> <span class="n">acorr_ljungbox</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">normaltest</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/evgomez98/wind_speed/main/wind_dataset.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Traemos los dos modelos en cuestión, iniciando por la división de los datos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tau_test</span> <span class="o">=</span> <span class="mi">365</span>
<span class="n">tau_val</span>  <span class="o">=</span> <span class="mi">365</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;WIND&#39;</span><span class="p">][:</span><span class="o">-</span><span class="p">(</span><span class="n">tau_val</span> <span class="o">+</span> <span class="n">tau_test</span><span class="p">)]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">val</span>   <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;WIND&#39;</span><span class="p">][</span><span class="o">-</span><span class="p">(</span><span class="n">tau_val</span> <span class="o">+</span> <span class="n">tau_test</span><span class="p">):</span><span class="o">-</span><span class="n">tau_test</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">test</span>  <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;WIND&#39;</span><span class="p">][</span><span class="o">-</span><span class="n">tau_test</span><span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span><span class="si">}</span><span class="s2">, Validation: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="si">}</span><span class="s2">, Test: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train: 5844, Validation: 365, Test: 365
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tlsmooth</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">delta_</span><span class="p">,</span> <span class="n">y_tilde_start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lambda_start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="n">Qt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    <span class="n">Dt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    <span class="n">y_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    <span class="n">lambd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    
    <span class="n">lambd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">lambda_start</span>
    <span class="k">if</span> <span class="n">y_tilde_start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y_tilde</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_tilde</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_tilde_start</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_tilde</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">Qt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_</span> <span class="o">*</span> <span class="n">err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">delta_</span><span class="p">)</span> <span class="o">*</span> <span class="n">Qt</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">Dt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">err</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">delta_</span><span class="p">)</span> <span class="o">*</span> <span class="n">Dt</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">lambd</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">Qt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">Dt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">y_tilde</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lambd</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">y_tilde</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">y_tilde</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">err</span><span class="p">,</span> <span class="n">Qt</span><span class="p">,</span> <span class="n">Dt</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">delta_</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">train_result</span> <span class="o">=</span> <span class="n">tlsmooth</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">delta_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Metricas del TLsmoother</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="s1">&#39;TLsmooth test Δ = 0.8&#39;</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_result</span> <span class="o">=</span> <span class="n">tlsmooth</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">delta_</span><span class="p">)</span>

<span class="n">test_pred_TLS</span> <span class="o">=</span> <span class="n">test_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred_TLS</span><span class="p">)</span>
<span class="n">sse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_TLS</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_TLS</span><span class="p">)</span> <span class="o">/</span> <span class="n">test</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">msd</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred_TLS</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred_TLS</span><span class="p">)</span> 

<span class="n">ljung_box</span> <span class="o">=</span> <span class="n">acorr_ljungbox</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_TLS</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="p">[</span><span class="mi">90</span><span class="p">],</span> <span class="n">return_df</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">normality_test_stat</span><span class="p">,</span> <span class="n">normality_p_value</span> <span class="o">=</span> <span class="n">normaltest</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_TLS</span><span class="p">)</span>

<span class="n">df_acc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mae</span><span class="p">],</span>
                    <span class="s1">&#39;SSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">sse</span><span class="p">],</span>
                    <span class="s1">&#39;MAPE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mape</span><span class="p">],</span>
                    <span class="s1">&#39;MSD&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">msd</span><span class="p">],</span>
                    <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">r2</span><span class="p">],</span>
                    <span class="s1">&#39;Ljung-Box (p-value)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ljung_box</span><span class="p">[</span><span class="s1">&#39;lb_pvalue&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                    <span class="s1">&#39;Normalidad (p-value)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">normality_p_value</span><span class="p">]},</span>
                    <span class="n">index</span><span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="n">df_acc</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MAE</th>
      <th>SSE</th>
      <th>MAPE</th>
      <th>MSD</th>
      <th>R2</th>
      <th>Ljung-Box (p-value)</th>
      <th>Normalidad (p-value)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>TLsmooth test Δ = 0.8</th>
      <td>0.65569</td>
      <td>399.159125</td>
      <td>8.631877</td>
      <td>1.093587</td>
      <td>0.96004</td>
      <td>0.002216</td>
      <td>4.763272e-10</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.tsaplots</span> <span class="kn">import</span> <span class="n">plot_acf</span><span class="p">,</span> <span class="n">plot_pacf</span> 
<span class="kn">import</span> <span class="nn">statsmodels.tsa.api</span> <span class="k">as</span> <span class="nn">smtsa</span>
<span class="kn">import</span> <span class="nn">statsmodels.tsa.arima.model</span> <span class="k">as</span> <span class="nn">arima_model</span>
<span class="kn">from</span> <span class="nn">statsmodels.tsa.arima.model</span> <span class="kn">import</span> <span class="n">ARIMA</span>

<span class="n">best_order</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ARIMA</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">best_order</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">arima_rolling</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">best_order</span><span class="p">):</span>
    <span class="n">history</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">ARIMA</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">best_order</span><span class="p">)</span>
        <span class="n">model_fit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model_fit</span><span class="o">.</span><span class="n">forecast</span><span class="p">()</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> 
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>  
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Predicted=</span><span class="si">{</span><span class="n">yhat</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, Expected=</span><span class="si">{</span><span class="n">obs</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">test_pred_ARMA</span>  <span class="o">=</span> <span class="n">arima_rolling</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">best_order</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=13.580, Expected=14.710
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=13.275, Expected=17.080
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=14.613, Expected=21.960
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=17.140, Expected=8.380
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=9.518, Expected=13.880
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=13.906, Expected=11.290
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=10.866, Expected=8.080
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=9.717, Expected=8.120
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=9.446, Expected=17.330
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=14.245, Expected=16.620
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=13.162, Expected=19.410
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=15.607, Expected=11.580
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=11.108, Expected=9.870
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted=11.099, Expected=4.170
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="s1">&#39;ARMA rolling (1, 0, 2) test&#39;</span> 

<span class="n">test_pred_ARMA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_pred_ARMA</span><span class="p">)</span>        

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred_ARMA</span><span class="p">)</span>
<span class="n">sse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_ARMA</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_ARMA</span><span class="p">)</span> <span class="o">/</span> <span class="n">test</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">msd</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred_ARMA</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred_ARMA</span><span class="p">)</span> 

<span class="n">ljung_box</span> <span class="o">=</span> <span class="n">acorr_ljungbox</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_ARMA</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">return_df</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">normality_test_stat</span><span class="p">,</span> <span class="n">normality_p_value</span> <span class="o">=</span> <span class="n">normaltest</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred_ARMA</span><span class="p">)</span>

<span class="n">df_acc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mae</span><span class="p">],</span>
                    <span class="s1">&#39;SSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">sse</span><span class="p">],</span>
                    <span class="s1">&#39;MAPE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mape</span><span class="p">],</span>
                    <span class="s1">&#39;MSD&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">msd</span><span class="p">],</span>
                    <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">r2</span><span class="p">],</span>
                    <span class="s1">&#39;Ljung-Box (p-value)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ljung_box</span><span class="p">[</span><span class="s1">&#39;lb_pvalue&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                    <span class="s1">&#39;Normalidad (p-value)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">normality_p_value</span><span class="p">]},</span>
                    <span class="n">index</span><span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="n">df_acc</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MAE</th>
      <th>SSE</th>
      <th>MAPE</th>
      <th>MSD</th>
      <th>R2</th>
      <th>Ljung-Box (p-value)</th>
      <th>Normalidad (p-value)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ARMA rolling (1, 0, 2) test</th>
      <td>3.326124</td>
      <td>6425.376786</td>
      <td>61.322275</td>
      <td>17.603772</td>
      <td>0.356756</td>
      <td>0.725282</td>
      <td>0.001373</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Para el emsamble  del modelo se define la función que tendrá como entrada las predicciones de ambos modelos y el factor de ponderación <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<p>Ahora, a través de un <code class="docutils literal notranslate"><span class="pre">loop</span></code> atomatizamos la busqueda de <span class="math notranslate nohighlight">\(\alpha\)</span> que minimice el error absoluto promedio.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">ensamble_pred</span><span class="p">(</span><span class="n">ses_pred</span><span class="p">,</span> <span class="n">arima_pred</span><span class="p">,</span> <span class="n">test</span><span class="p">):</span>
    <span class="n">ses_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ses_pred</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">ses_pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ses_pred</span><span class="p">)</span>
    <span class="n">arima_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arima_pred</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">arima_pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arima_pred</span><span class="p">)</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">test</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
    
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)):</span>
        <span class="n">error_ses</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">ses_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">error_arima</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">arima_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">error_arima</span> <span class="o">&gt;</span> <span class="n">error_ses</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># priorizar ARIMA</span>
        <span class="k">elif</span> <span class="n">error_ses</span> <span class="o">&gt;</span> <span class="n">error_arima</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># priorizar SES</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> 
        
        <span class="n">pred</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">ses_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">arima_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">final_ensamble_pred</span>  <span class="o">=</span> <span class="n">ensamble_pred</span><span class="p">(</span><span class="n">test_pred_TLS</span><span class="p">,</span> <span class="n">test_pred_ARMA</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora probamos el modelo ensamblado:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="s1">&#39;Modelo Ensamblado Adaptativo&#39;</span>
<span class="n">test_pred</span> <span class="o">=</span> <span class="n">final_ensamble_pred</span> 

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">)</span>
<span class="n">sse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred</span><span class="p">)</span> <span class="o">/</span> <span class="n">test</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">msd</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">test</span><span class="p">,</span><span class="n">test_pred</span><span class="p">)</span> 

<span class="n">ljung_box</span> <span class="o">=</span> <span class="n">acorr_ljungbox</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="p">[</span><span class="mi">90</span><span class="p">],</span> <span class="n">return_df</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">normality_test_stat</span><span class="p">,</span> <span class="n">normality_p_value</span> <span class="o">=</span> <span class="n">normaltest</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">test_pred</span><span class="p">)</span>

<span class="n">df_acc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mae</span><span class="p">],</span>
                    <span class="s1">&#39;SSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">sse</span><span class="p">],</span>
                    <span class="s1">&#39;MAPE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mape</span><span class="p">],</span>
                    <span class="s1">&#39;MSD&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">msd</span><span class="p">],</span>
                    <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">r2</span><span class="p">],</span>
                    <span class="s1">&#39;Ljung-Box (p-value)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ljung_box</span><span class="p">[</span><span class="s1">&#39;lb_pvalue&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                    <span class="s1">&#39;Normalidad (p-value)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">normality_p_value</span><span class="p">]},</span>
                    <span class="n">index</span><span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="n">df_acc</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MAE</th>
      <th>SSE</th>
      <th>MAPE</th>
      <th>MSD</th>
      <th>R2</th>
      <th>Ljung-Box (p-value)</th>
      <th>Normalidad (p-value)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Modelo Ensamblado Adaptativo</th>
      <td>0.56752</td>
      <td>298.887579</td>
      <td>7.769357</td>
      <td>0.81887</td>
      <td>0.970078</td>
      <td>0.007334</td>
      <td>8.116338e-09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Se concluye que las metricas de error asociadas al modelo emsamblado son mejores que los dos modelos bench mark utilizados como insumo, sin embargo los supuestos de normaidad y no autocorrelación no se cumplen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="k">def</span> <span class="nf">plot_model</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">final_ensamble_pred</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
  
    <span class="n">train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
    <span class="n">test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">final_ensamble_pred</span><span class="p">)</span>

   
    <span class="n">train_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">))</span>
    <span class="n">val_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">val</span><span class="p">))</span>
    <span class="n">test_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
    <span class="n">test_pred_index</span> <span class="o">=</span> <span class="n">test_index</span>  

    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">val</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Validation&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">test_index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">test</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">test_pred_index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Prediction&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">dash</span><span class="o">=</span><span class="s1">&#39;dash&#39;</span><span class="p">)))</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
        <span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Observaciones&quot;</span><span class="p">,</span>
        <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Valores&quot;</span><span class="p">,</span>
        <span class="n">legend</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">traceorder</span><span class="o">=</span><span class="s2">&quot;normal&quot;</span><span class="p">),</span>
        <span class="n">plot_bgcolor</span><span class="o">=</span><span class="s1">&#39;rgba(0,0,0,0)&#39;</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span>
    <span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">final_ensamble_pred</span><span class="p">,</span> <span class="s1">&#39;Modelo Emsamblado Adaptativo&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>                            <div id="ba2e5094-85f7-4772-bc04-84b9f0ae8069" class="plotly-graph-div" style="height:600px; width:1000px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ba2e5094-85f7-4772-bc04-84b9f0ae8069")) {                    Plotly.newPlot(                        "ba2e5094-85f7-4772-bc04-84b9f0ae8069",                        [{"line":{"color":"blue"},"mode":"lines","name":"Train","x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843],"y":[13.67,11.5,11.25,8.63,11.92,10.67,9.17,14.29,8.04,11.42,7.54,15.54,4.63,3.08,2.5,9.25,13.42,17.83,3.71,3.17,8.04,9.38,13.7,20.38,9.25,14.33,16.79,12.25,22.0,14.5,14.88,10.17,10.88,8.5,12.54,13.96,19.17,14.21,16.5,14.37,11.08,17.5,11.71,16.08,15.46,5.17,3.42,6.17,10.88,9.62,7.25,3.75,8.25,16.38,8.17,13.08,21.17,12.75,14.75,10.25,10.88,11.29,6.21,5.5,2.83,3.63,1.71,3.75,8.79,10.34,17.12,13.96,8.12,11.54,2.75,13.0,13.17,7.75,15.41,8.17,8.0,10.88,8.46,10.75,13.75,6.75,10.88,16.62,14.79,11.92,11.67,5.25,5.5,6.0,9.25,5.0,5.33,8.71,9.96,5.21,5.66,6.54,8.0,4.29,2.46,2.88,4.46,5.66,12.96,12.25,9.5,7.46,10.71,5.25,13.75,6.0,6.25,2.67,4.04,5.09,9.96,10.46,9.5,5.96,9.13,16.83,18.21,18.58,7.25,3.33,3.67,5.17,5.21,10.04,5.91,8.38,5.83,5.04,6.83,4.71,3.17,2.42,3.13,4.0,6.79,9.33,4.92,5.41,11.71,5.83,6.58,9.75,7.92,10.41,6.21,3.42,4.21,8.17,9.54,8.29,9.71,6.25,9.17,4.67,5.66,8.25,10.54,12.96,13.7,10.37,7.92,10.54,13.29,14.21,14.88,11.87,8.38,10.54,8.79,8.33,4.92,6.71,5.83,15.75,14.92,10.08,7.08,14.54,14.04,9.46,2.67,7.41,9.08,16.83,11.83,10.25,13.13,7.08,4.96,3.5,3.04,2.21,4.75,2.21,3.58,12.29,18.79,7.87,9.67,5.79,6.04,3.75,4.21,8.87,11.92,13.54,10.54,7.87,3.88,10.63,11.25,4.67,6.83,6.83,10.92,15.09,9.42,12.96,10.21,19.04,13.21,12.58,19.0,12.08,8.17,5.66,10.92,9.79,12.12,9.08,14.12,4.58,2.92,4.63,3.83,5.88,1.25,9.38,14.09,7.87,3.17,6.92,4.83,8.63,11.08,11.75,12.75,13.0,20.79,7.87,4.17,8.17,4.79,6.46,4.12,6.08,5.91,8.92,12.46,8.08,9.13,9.21,10.96,5.83,8.71,4.04,3.5,5.63,12.96,6.87,10.17,9.04,11.75,12.71,5.58,9.92,2.96,9.0,14.58,22.17,21.84,13.21,12.08,7.5,17.58,16.21,17.96,18.34,9.79,7.79,9.75,9.42,14.88,15.25,13.08,15.83,11.21,8.5,10.79,8.21,9.29,6.58,8.21,7.71,12.12,13.04,21.87,15.71,6.0,2.29,3.13,9.29,9.33,13.04,7.0,9.17,15.41,16.17,11.83,6.34,5.83,12.79,15.29,16.5,7.25,12.21,7.67,13.79,17.83,12.62,8.87,19.83,10.04,18.84,25.62,16.83,16.66,9.38,10.71,6.92,9.54,4.79,3.54,3.63,4.33,13.59,18.12,8.5,11.92,17.12,2.92,8.96,15.0,17.29,10.04,10.41,8.46,4.58,5.5,15.29,8.46,14.29,17.71,15.29,14.29,25.96,23.54,19.7,12.42,20.38,19.75,22.79,12.46,16.08,10.29,17.29,23.25,12.71,17.25,18.29,7.41,5.17,3.63,2.17,13.33,22.42,15.71,13.17,15.96,18.88,15.75,18.75,25.0,8.21,12.08,14.96,24.41,29.54,15.79,11.71,23.45,28.84,14.12,8.29,7.92,1.46,7.04,15.29,20.21,18.12,22.63,25.62,26.87,13.88,7.96,9.59,6.71,5.33,5.79,8.0,29.58,18.0,9.0,3.83,10.92,7.71,6.83,4.0,16.54,12.58,7.71,5.79,9.04,7.67,6.71,5.96,9.42,3.5,19.41,14.88,9.92,13.54,10.83,13.75,16.42,19.79,20.83,10.67,17.83,22.42,17.08,22.5,18.25,16.21,12.04,14.12,3.79,6.58,11.42,13.83,15.34,14.83,11.71,6.21,6.17,3.58,8.58,8.96,5.79,5.83,7.67,6.42,5.17,5.37,5.41,2.25,4.0,3.5,5.83,8.79,4.88,14.09,11.34,7.04,5.96,11.04,10.46,8.75,13.79,9.83,23.25,18.05,11.21,17.12,12.42,11.63,16.0,16.42,11.67,8.0,7.46,9.92,6.83,6.75,7.17,7.83,5.91,3.17,3.0,3.33,8.21,6.08,3.83,5.41,8.29,6.5,7.96,9.08,9.62,14.0,2.79,8.42,10.37,21.5,23.42,12.54,16.04,13.67,16.66,25.29,18.79,14.33,11.92,12.79,6.42,7.12,11.17,17.5,12.87,10.08,7.79,5.46,3.13,4.67,4.63,7.5,5.71,4.75,5.54,8.0,5.79,1.87,4.54,12.33,13.04,9.17,12.96,11.29,5.88,4.38,6.34,10.0,3.67,2.88,6.58,12.17,9.54,4.46,8.38,8.42,14.25,10.75,5.63,10.5,7.12,12.25,14.29,18.5,9.38,5.88,8.67,10.41,11.38,11.25,7.54,10.5,11.25,14.21,15.09,17.04,21.09,15.5,22.13,14.29,5.54,7.75,6.42,5.37,7.41,5.0,6.17,6.46,7.5,5.25,11.63,8.04,18.79,9.59,8.38,7.54,6.13,7.25,7.92,11.75,10.58,6.79,2.88,4.63,2.29,6.58,8.33,4.88,10.83,12.0,8.42,11.29,13.79,16.54,13.29,7.12,7.33,11.54,8.29,6.25,8.04,8.46,4.88,4.83,7.79,4.96,2.5,4.88,3.92,4.38,6.29,11.63,5.58,3.79,6.34,4.92,6.54,8.5,11.79,11.04,15.63,15.92,11.96,20.75,12.75,10.46,6.04,6.83,10.92,19.75,8.29,3.5,4.42,12.58,16.25,14.75,5.5,7.29,13.7,9.42,10.41,18.0,16.46,10.37,10.54,8.42,8.38,12.96,11.17,4.0,5.96,5.09,4.88,7.41,4.71,8.42,12.96,10.13,4.79,8.96,4.17,13.13,23.67,20.25,12.67,16.42,19.67,14.0,20.0,28.91,16.54,9.59,15.92,9.87,16.08,10.41,5.71,9.33,4.92,7.0,12.38,11.92,7.79,13.62,28.79,27.79,23.45,20.75,13.83,8.54,13.08,20.91,16.75,16.83,21.67,19.38,9.79,3.96,5.21,10.46,9.17,12.58,15.37,21.5,23.79,19.62,13.83,11.92,2.17,3.04,8.92,8.38,1.63,1.33,7.41,19.62,11.83,14.04,15.16,8.67,9.13,22.0,20.25,11.83,10.37,16.88,15.67,6.79,5.21,12.83,19.17,15.5,13.54,7.83,10.34,14.54,9.92,8.5,5.5,9.33,13.75,11.46,19.25,12.5,16.46,16.21,15.83,14.46,14.42,20.21,9.96,17.88,17.5,13.33,17.33,5.66,6.67,11.46,13.92,14.25,16.62,13.17,12.42,9.54,7.54,4.33,4.17,4.79,19.75,19.55,7.62,13.92,9.83,8.67,9.38,6.83,11.79,8.96,8.71,13.0,20.04,20.5,18.5,10.41,11.67,7.25,9.54,16.0,15.21,16.46,8.87,9.67,8.0,9.92,9.5,21.54,19.58,13.42,6.0,6.67,4.83,5.71,12.12,11.42,11.75,12.33,14.46,15.37,12.96,13.37,14.96,13.67,18.75,11.67,13.88,23.09,12.25,15.59,20.33,13.42,12.83,14.42,13.96,19.33,11.38,14.5,7.21,7.62,7.0,4.96,8.38,9.29,5.13,5.63,10.92,8.25,6.13,10.92,14.46,17.67,9.79,6.0,6.29,3.58,4.25,8.38,5.75,2.92,4.25,4.92,4.83,7.17,8.29,5.88,17.83,8.92,8.67,10.34,11.83,11.87,12.04,8.17,12.71,13.04,12.17,13.5,11.58,8.54,5.09,6.46,8.5,5.17,2.67,6.46,10.92,13.96,8.17,4.46,4.79,5.54,14.71,12.04,10.29,9.75,6.5,10.04,6.5,7.29,6.5,7.87,11.17,11.0,6.5,5.5,5.91,8.21,4.33,4.33,5.41,4.08,2.62,3.13,10.5,7.0,12.58,13.5,7.96,14.09,11.25,8.38,6.58,6.67,9.21,17.67,15.67,6.92,7.79,11.08,9.46,7.62,13.13,13.59,9.0,19.62,14.62,7.41,8.33,11.12,9.59,10.34,8.96,11.25,5.96,9.25,10.34,14.67,13.13,14.04,3.71,2.54,4.29,9.79,9.33,5.91,4.75,7.83,5.41,6.75,5.5,2.54,3.21,5.09,14.67,13.37,23.04,15.0,10.17,17.88,12.5,15.71,10.54,14.21,14.25,9.04,8.71,11.17,14.33,15.25,14.42,9.83,9.42,14.21,3.83,11.67,7.46,10.37,12.96,15.09,13.42,13.88,9.79,12.92,4.83,6.04,11.21,15.37,12.04,8.21,15.92,5.25,8.25,6.0,7.33,16.13,11.12,3.0,7.62,13.0,6.58,19.83,18.29,25.92,14.42,12.08,2.92,6.29,13.75,22.5,10.08,7.17,24.71,10.21,13.17,14.62,18.41,14.42,5.91,10.41,6.46,6.42,13.79,16.0,21.29,16.08,10.71,13.21,8.92,1.83,3.46,12.17,17.54,16.96,13.04,9.08,6.5,4.92,6.42,7.5,10.58,8.38,10.88,6.63,12.08,18.08,12.21,8.29,10.54,12.87,14.29,17.29,18.29,14.12,22.63,16.13,6.5,3.67,2.54,2.92,4.08,2.67,2.71,4.42,7.54,15.04,8.71,11.12,11.34,12.25,15.29,2.62,7.58,1.92,3.0,9.92,6.34,9.08,7.12,11.08,15.87,17.25,19.0,22.92,17.67,15.34,16.79,15.5,7.25,5.63,5.88,7.21,7.54,10.83,8.5,9.67,20.3,17.04,13.88,12.96,12.83,24.0,16.38,9.67,18.29,19.25,8.92,15.04,11.08,16.25,13.79,8.46,5.88,8.87,6.04,4.83,7.83,7.67,5.88,10.08,9.5,10.29,10.67,15.16,15.37,12.87,13.21,6.54,17.41,23.13,22.54,15.37,11.34,8.71,6.08,11.0,7.58,15.37,6.0,9.67,8.92,7.71,7.71,10.58,12.04,10.63,11.83,12.75,6.58,7.33,7.29,11.63,18.05,8.54,10.04,14.37,15.96,19.04,10.71,9.17,12.79,12.87,4.75,9.25,5.41,9.04,9.79,7.83,12.29,7.21,11.42,11.63,15.09,14.5,11.08,9.79,20.08,19.41,15.54,9.75,16.66,19.7,15.96,15.92,15.04,14.37,11.54,9.92,5.25,5.66,7.08,13.46,7.33,7.54,6.96,10.71,10.92,11.08,11.29,6.13,8.87,10.54,8.17,3.08,13.54,17.37,10.67,7.67,15.5,11.12,8.25,10.04,10.71,8.58,9.62,11.21,6.75,8.63,13.25,15.37,13.21,6.17,5.91,13.25,10.25,8.96,3.67,4.63,4.29,3.92,8.0,10.41,5.21,8.92,14.88,7.92,9.87,6.58,5.21,8.42,6.42,19.41,16.21,14.5,12.25,13.42,6.54,9.54,13.54,11.58,4.29,4.08,4.63,7.79,2.5,1.96,4.63,6.83,9.96,8.17,3.96,15.09,11.96,6.71,10.13,13.79,16.54,14.04,8.71,4.42,7.67,6.83,9.21,6.96,4.88,3.08,3.63,2.46,5.09,3.04,2.79,7.08,13.42,15.09,10.67,5.71,4.25,9.42,13.62,15.34,13.0,8.04,7.12,11.34,9.25,4.38,7.21,11.29,10.0,2.04,2.46,5.63,8.58,10.37,12.33,13.29,14.58,10.75,4.5,9.17,9.42,10.13,8.08,9.29,10.75,15.12,10.17,8.33,14.21,6.83,15.04,10.58,9.08,6.25,6.87,5.88,3.67,3.08,6.96,7.67,6.87,10.41,16.46,18.71,14.83,7.12,15.37,8.71,6.75,9.08,9.13,12.04,10.13,5.58,4.17,5.17,5.75,5.25,12.0,14.71,10.37,6.25,6.25,2.54,2.46,1.92,1.75,6.21,2.46,6.17,8.54,9.67,8.0,5.88,9.21,7.71,3.46,2.96,8.96,14.04,16.13,24.08,17.88,17.33,9.62,12.42,11.29,9.25,6.46,11.63,13.54,18.16,8.5,13.21,16.08,15.41,14.71,11.12,12.42,15.04,15.79,9.54,16.42,19.38,16.75,20.88,13.54,7.17,14.0,12.75,15.71,8.87,7.83,9.87,8.21,3.21,6.29,10.21,14.37,15.04,10.34,10.17,10.79,8.71,10.37,6.96,17.88,20.3,19.83,10.25,7.79,8.63,7.71,7.62,14.79,13.17,13.21,21.84,19.62,18.12,17.21,23.45,20.12,17.08,18.84,28.5,16.96,10.63,20.12,10.63,11.34,9.92,5.58,5.5,8.87,9.33,10.29,13.29,15.09,13.88,8.63,8.83,4.21,3.63,5.66,4.79,9.38,4.79,6.0,7.29,12.87,21.04,18.63,9.29,8.33,5.88,9.42,4.25,13.46,15.92,11.83,10.0,7.41,12.42,4.79,10.83,8.46,11.0,17.33,8.79,14.83,22.46,11.54,8.46,10.63,8.46,9.33,18.34,12.42,13.17,8.58,9.21,15.92,20.3,6.63,12.96,7.12,12.58,12.08,8.38,10.5,14.04,17.12,14.04,16.66,7.41,5.71,10.83,10.67,8.67,8.12,8.71,11.34,4.88,9.17,8.08,9.92,12.17,18.0,19.55,18.66,10.37,12.46,15.34,16.21,27.08,21.87,20.38,8.04,7.5,4.79,8.67,8.08,10.88,21.5,12.21,8.33,6.92,10.25,12.12,14.0,7.46,11.0,15.25,7.08,11.58,17.46,15.25,4.79,3.79,8.46,7.41,3.37,3.25,5.88,6.83,9.87,4.08,7.17,12.62,10.21,12.83,8.96,7.87,9.54,12.38,9.13,8.5,9.17,7.33,4.17,1.83,3.79,6.5,3.46,4.58,2.5,4.42,3.54,4.42,6.29,5.46,8.5,7.17,15.67,11.17,13.42,24.21,13.83,13.46,5.17,10.92,21.04,16.92,15.12,17.0,8.83,11.63,4.12,8.5,8.54,10.17,9.38,7.79,9.92,4.88,7.75,9.42,7.04,8.04,10.08,5.71,4.5,5.96,8.04,5.41,4.88,10.46,8.83,4.79,7.21,4.79,6.96,10.96,7.79,10.67,12.96,14.09,18.71,5.33,13.29,9.92,7.71,5.63,11.92,17.54,12.33,8.63,3.58,6.54,9.75,11.08,11.17,6.5,7.12,7.21,4.21,7.87,12.67,10.96,14.17,14.88,16.83,11.87,16.42,15.71,7.79,8.58,16.92,15.25,11.87,9.87,7.75,3.29,10.63,13.54,11.29,3.96,5.33,7.29,16.33,14.21,10.92,3.42,2.71,4.25,6.42,9.54,12.71,13.04,9.25,9.54,12.21,8.25,4.21,5.96,4.29,3.42,5.37,2.5,11.63,15.63,8.0,4.67,9.79,6.79,2.13,0.42,4.96,10.71,11.79,9.04,4.0,3.0,3.29,6.75,10.08,5.29,4.96,5.33,9.21,10.46,11.87,8.96,7.87,6.83,7.17,6.08,17.0,20.0,15.12,21.62,19.21,29.17,12.04,11.42,6.04,4.83,16.0,11.58,9.59,9.38,6.96,14.83,6.75,8.5,11.63,11.34,27.58,25.33,15.92,9.75,15.75,10.75,10.67,20.0,7.46,13.75,9.33,16.33,9.5,20.33,19.38,9.96,19.25,15.96,6.46,15.63,18.63,11.0,13.46,24.0,14.29,11.54,8.92,14.29,11.63,8.54,8.21,17.92,7.5,11.71,8.96,9.42,9.38,11.04,7.12,7.38,5.79,6.38,6.58,12.5,17.41,9.59,21.79,17.46,7.83,15.46,12.83,10.96,10.5,12.5,13.37,19.21,19.55,16.46,12.29,10.83,6.29,4.71,2.79,8.5,14.33,12.33,4.75,8.54,1.13,8.08,20.46,19.08,12.25,17.46,16.79,14.33,3.79,13.75,13.54,10.96,10.96,19.33,10.63,8.04,4.58,6.13,13.29,11.42,12.67,11.63,12.42,16.46,10.96,13.7,13.59,10.04,11.92,8.71,13.59,17.79,12.0,21.84,22.95,23.63,14.79,9.38,13.92,10.46,9.71,10.29,12.17,16.04,9.04,16.13,22.29,20.62,11.25,11.21,8.63,8.21,6.87,14.83,7.21,5.25,8.87,9.96,13.17,20.54,20.83,10.92,21.29,28.62,14.54,10.92,13.59,12.08,14.42,11.25,10.58,21.25,17.37,4.88,9.33,11.42,18.63,10.41,14.21,17.71,24.71,21.79,20.62,10.41,10.71,16.66,11.71,11.96,7.87,15.29,12.96,11.08,9.21,13.75,20.88,13.54,13.29,4.75,4.54,1.96,7.38,15.34,11.34,16.0,9.38,5.96,12.62,10.5,6.34,5.79,11.54,11.79,6.25,6.75,9.87,9.17,12.0,9.25,15.96,24.41,15.04,10.75,14.96,11.38,8.17,6.54,5.66,2.62,0.83,4.54,4.79,8.58,16.54,10.41,5.33,2.5,5.04,7.96,12.08,7.04,2.5,2.29,5.17,8.67,10.34,8.63,5.58,6.34,4.21,4.71,3.63,8.92,14.42,9.71,10.75,17.0,11.12,4.46,6.42,3.71,4.83,2.88,1.46,1.63,6.96,6.79,5.83,7.67,7.5,7.58,13.79,14.83,10.08,10.25,6.17,4.75,1.38,5.46,9.38,7.17,7.79,13.21,12.46,9.42,10.04,5.46,11.21,10.71,13.17,3.33,5.41,5.41,7.08,10.63,9.5,3.17,5.83,8.25,12.79,17.46,8.58,6.54,9.67,2.88,4.17,8.87,7.0,3.08,4.63,3.46,8.46,6.58,3.21,5.09,6.38,9.17,8.17,8.96,10.25,6.58,6.13,7.62,12.5,14.37,14.67,15.67,16.38,6.75,3.83,7.96,11.96,3.96,13.04,15.41,18.21,16.29,6.13,2.04,1.71,1.21,1.71,1.83,2.71,3.63,2.33,3.88,1.87,4.79,3.88,0.42,2.33,4.25,4.33,21.29,17.79,7.58,9.71,2.0,4.42,3.79,2.83,7.12,13.17,4.75,2.83,5.79,5.63,11.96,10.13,5.88,8.5,9.67,5.91,8.12,7.62,6.0,11.08,14.79,7.75,5.88,5.46,9.92,18.58,13.79,5.17,5.63,17.67,19.87,6.17,7.38,8.71,6.75,7.96,11.17,11.42,12.12,23.09,21.87,13.92,8.42,2.92,3.54,6.71,8.17,5.33,7.96,12.12,14.79,16.75,15.37,19.38,21.75,30.37,28.16,14.5,11.75,14.96,12.21,15.09,25.12,23.75,14.58,11.21,20.04,13.33,9.87,11.29,14.25,26.0,18.5,23.04,12.58,17.16,17.37,16.88,10.67,4.96,10.41,9.67,9.62,16.5,5.25,13.5,11.38,8.29,5.66,5.63,6.87,10.63,8.63,6.34,7.33,11.0,9.59,9.54,9.96,6.21,3.92,7.75,14.79,9.0,13.67,10.21,13.54,13.29,10.08,6.71,11.79,7.75,12.38,9.25,13.13,12.92,16.17,14.67,16.88,20.12,10.0,9.96,10.41,7.96,6.25,5.66,4.5,9.38,11.0,12.54,14.62,26.54,7.17,4.54,10.5,19.08,14.83,16.17,11.34,16.33,15.09,12.04,13.75,22.08,23.04,20.88,21.92,18.71,14.71,15.46,19.17,16.75,9.25,13.5,22.83,26.54,11.92,16.71,22.29,17.92,14.71,25.25,17.08,19.95,17.21,20.38,17.04,15.04,16.29,17.12,20.38,19.62,18.16,15.09,11.5,6.96,11.87,15.09,13.54,15.83,19.67,18.54,15.92,15.16,18.05,17.12,11.83,7.79,8.71,4.54,9.38,6.29,12.79,7.08,17.71,18.12,12.33,10.88,7.5,10.71,6.83,9.38,7.38,6.46,5.58,13.13,13.83,8.75,16.46,17.58,15.29,9.21,9.29,10.71,6.46,2.29,9.5,12.71,12.67,9.83,15.09,7.79,8.5,9.5,15.09,18.58,14.62,16.83,11.34,12.79,8.83,9.0,3.63,4.25,6.29,2.83,4.79,3.88,5.09,10.0,8.75,10.88,16.38,8.67,6.13,8.17,3.92,5.0,4.96,2.75,4.67,1.79,2.46,3.04,2.79,8.63,15.16,11.34,8.33,6.08,7.92,6.21,3.54,5.37,14.42,12.17,6.5,6.0,11.58,13.29,4.96,5.83,8.46,9.25,12.58,10.0,8.25,4.25,3.46,5.5,5.0,5.5,7.04,4.71,7.12,6.63,6.25,4.04,3.21,3.54,11.29,7.12,10.71,6.25,7.58,8.08,8.12,3.83,7.62,5.5,11.46,11.71,6.71,6.42,11.63,7.38,4.5,4.88,11.5,15.71,9.21,15.5,11.54,10.96,8.75,9.83,4.33,3.37,5.17,3.54,3.92,2.83,3.79,4.54,4.71,5.71,10.54,12.96,3.96,8.87,15.0,19.67,14.54,22.34,12.75,5.41,4.54,5.79,3.5,9.46,8.58,3.67,1.83,2.96,4.25,9.62,10.08,11.46,9.33,7.83,4.92,10.08,11.0,11.75,9.54,9.33,11.21,7.21,17.83,15.34,15.92,18.0,15.09,6.83,13.46,15.5,11.38,14.58,4.42,10.0,8.38,15.67,19.33,13.25,9.42,16.21,10.04,14.29,14.42,7.38,7.25,11.71,10.34,21.29,22.0,7.75,11.79,10.04,8.29,14.46,11.29,19.12,10.67,7.17,12.79,10.34,5.91,4.46,10.83,12.54,17.62,5.5,12.38,14.92,14.58,6.54,2.17,3.04,2.71,0.58,3.83,4.25,0.54,3.42,8.87,9.29,11.08,17.88,13.88,11.29,5.88,4.83,10.17,16.62,19.75,12.25,12.62,15.79,12.29,8.83,12.83,10.29,6.67,9.21,16.21,6.13,1.63,8.04,10.34,10.58,14.88,18.41,18.88,14.17,19.25,10.08,16.13,16.25,10.63,14.58,11.75,18.16,12.67,17.0,7.71,9.75,9.87,7.12,18.0,14.88,10.96,4.42,10.0,10.0,22.04,19.41,15.54,20.12,16.42,8.08,3.46,0.46,0.54,8.75,13.04,19.79,14.5,12.29,8.75,17.62,16.92,16.0,15.59,13.46,9.29,15.04,6.67,8.71,5.63,1.75,3.88,10.04,14.71,6.42,9.92,14.71,5.71,6.17,7.58,3.63,3.0,2.37,12.12,14.92,8.46,8.29,9.87,4.83,6.21,11.83,5.54,4.63,7.08,7.54,10.75,16.92,16.92,10.75,5.79,8.12,11.08,11.34,6.25,15.71,13.04,14.5,19.95,24.5,21.09,18.66,11.25,7.67,13.29,13.62,4.0,13.67,20.5,19.79,6.04,6.38,8.5,16.71,21.12,17.0,13.29,6.58,10.08,9.33,5.71,4.63,4.96,4.83,5.75,8.5,7.38,11.75,11.17,11.0,14.29,11.46,10.58,8.38,3.17,7.96,7.58,6.17,5.66,12.62,8.17,9.29,4.17,6.21,3.88,3.17,6.96,3.63,6.92,15.96,7.33,13.5,9.96,16.25,12.0,11.83,5.58,6.92,16.58,8.79,4.46,5.37,3.33,5.29,3.92,4.12,5.04,10.08,3.46,3.67,3.29,3.0,4.25,4.63,3.04,4.96,4.54,3.29,7.83,6.13,11.08,6.5,3.54,2.96,1.87,4.67,3.67,3.33,5.33,3.42,3.13,5.13,5.96,5.41,9.38,10.41,6.0,13.75,8.0,11.75,7.12,5.37,8.21,9.13,12.0,2.71,12.38,15.67,7.96,3.13,2.42,4.12,3.79,3.46,10.17,6.67,5.25,8.08,12.12,11.08,10.04,4.58,2.46,5.37,4.46,2.83,8.87,7.04,2.42,2.25,4.54,2.13,3.04,5.96,2.96,2.37,3.63,2.58,2.42,2.71,6.13,7.04,7.87,8.83,7.38,2.83,5.29,5.96,13.33,10.37,6.46,4.88,7.41,9.33,10.46,10.0,6.38,6.08,4.46,8.46,7.25,7.46,6.25,5.13,6.0,8.54,8.42,4.0,6.42,7.38,5.5,10.41,5.75,7.38,10.13,13.42,4.92,3.46,2.46,1.42,11.04,14.96,10.79,4.17,5.13,13.29,18.75,13.59,8.29,14.37,8.96,8.0,9.67,16.92,17.33,13.54,12.83,17.33,14.0,5.46,4.79,2.04,4.46,1.75,3.21,8.17,12.58,14.42,20.41,16.46,9.71,11.08,12.08,7.62,15.59,13.88,7.5,3.04,1.21,8.96,6.04,1.13,3.54,4.63,9.29,9.04,6.42,3.25,8.92,20.33,9.59,6.38,15.12,16.75,17.67,9.71,9.87,17.96,7.38,17.75,16.33,14.12,12.58,4.04,3.83,1.54,9.08,11.96,10.88,10.08,12.38,11.83,14.09,4.04,5.5,5.09,10.92,8.67,12.42,10.04,11.63,1.63,8.29,8.96,6.46,5.83,10.63,6.71,2.29,7.5,12.04,1.96,10.29,4.5,20.88,17.37,6.87,7.5,9.42,24.62,9.25,9.42,9.59,7.83,9.87,10.46,10.83,7.58,6.83,8.5,7.75,7.96,11.25,7.67,10.63,13.46,8.38,14.92,12.04,4.42,10.29,3.0,6.0,4.12,17.29,14.04,17.12,8.42,12.54,5.04,3.42,3.42,8.92,14.09,12.33,12.46,5.13,9.92,19.55,12.29,15.29,11.5,8.92,6.75,10.79,17.5,16.08,11.71,9.67,8.67,17.58,12.17,8.08,7.08,6.34,4.58,4.08,10.88,27.71,21.67,14.29,17.21,8.25,5.91,1.08,4.92,6.38,12.46,8.67,7.62,6.83,11.21,7.17,2.62,1.08,7.96,9.0,3.92,8.5,11.58,15.34,5.33,5.75,16.17,15.0,14.67,7.33,4.08,9.5,14.92,9.75,7.04,6.5,5.37,5.33,10.54,13.17,14.54,18.5,9.42,5.46,4.96,3.42,7.87,13.75,6.38,5.04,14.37,9.59,17.62,19.04,9.96,14.5,17.16,9.87,3.71,8.67,6.92,7.41,12.83,12.71,8.67,9.25,14.29,4.17,10.25,6.17,4.71,3.33,4.08,11.38,5.71,4.08,2.83,2.04,4.71,6.87,9.42,8.58,2.29,5.66,5.46,7.54,4.21,10.29,6.71,12.46,3.92,1.96,4.17,3.25,7.96,9.96,7.12,5.17,7.62,6.79,8.29,8.42,6.63,4.71,9.62,8.58,3.42,2.96,0.21,4.42,2.88,3.75,6.5,3.29,2.92,3.79,4.63,10.46,10.37,11.29,8.29,13.42,11.04,9.38,4.17,3.63,7.0,5.71,9.83,9.54,5.37,5.96,6.25,7.92,6.0,4.21,12.96,9.17,7.08,12.21,8.17,7.62,12.38,13.92,8.92,6.67,2.42,3.71,9.0,7.87,12.25,16.25,9.59,8.0,10.71,3.5,2.71,7.96,1.5,4.71,3.42,8.42,7.21,7.04,2.5,6.63,8.0,9.59,7.12,5.63,8.21,5.58,1.54,5.5,6.17,7.21,2.79,2.42,0.71,3.5,3.96,4.92,10.08,13.59,10.21,10.79,9.54,7.87,12.17,15.79,13.0,8.92,5.91,5.21,4.12,6.34,2.08,3.83,3.5,4.92,2.17,2.42,4.0,10.04,5.25,6.83,1.92,3.25,9.21,6.92,7.71,3.75,3.79,3.54,10.5,16.17,7.38,10.37,10.13,9.96,17.12,6.46,16.92,5.75,11.38,7.33,16.42,7.38,8.08,7.25,8.04,10.79,14.0,4.25,1.17,3.58,10.63,11.92,11.5,16.13,9.87,14.37,4.33,1.04,1.38,1.5,3.33,13.88,13.42,12.54,9.29,4.04,8.25,7.46,11.58,8.25,13.37,19.41,4.75,10.0,9.67,2.88,11.29,18.16,10.71,12.87,5.79,7.04,5.41,4.29,9.79,11.96,10.25,16.25,19.25,11.96,2.96,2.58,14.96,8.92,9.5,8.21,13.17,12.38,9.5,7.71,8.96,12.87,12.21,7.21,8.0,11.04,15.54,7.25,5.5,7.5,5.66,2.88,11.34,14.92,12.92,7.12,17.67,10.13,11.79,8.75,14.46,13.33,10.5,4.96,6.21,5.25,3.04,9.71,14.29,16.79,14.58,9.0,8.58,5.13,8.92,12.33,10.34,8.58,19.08,13.54,4.63,5.5,6.13,11.29,9.62,6.0,3.83,16.54,12.87,16.79,9.96,13.33,8.54,11.29,11.08,2.25,7.04,5.17,13.75,10.83,5.66,17.54,18.25,13.96,14.29,5.63,5.33,7.17,17.54,15.5,17.0,10.83,6.5,10.13,8.75,5.91,5.41,7.67,15.79,13.5,16.54,17.04,21.96,16.88,15.29,15.29,10.88,4.33,7.67,6.42,14.62,10.63,10.25,13.62,8.83,8.5,10.29,11.21,7.25,10.08,16.88,14.37,16.75,7.29,8.33,14.92,22.04,19.7,12.25,15.29,7.38,5.25,11.34,8.17,4.25,11.58,11.92,13.46,16.38,19.0,15.16,13.08,10.83,12.04,11.34,14.25,14.42,8.0,7.46,8.92,6.25,3.96,14.58,4.96,5.66,9.71,13.17,11.75,11.0,11.08,15.41,14.58,19.55,16.21,6.92,19.41,12.12,7.33,13.83,9.04,8.29,12.75,16.66,9.71,9.25,6.79,3.37,5.91,7.92,12.5,10.21,5.75,7.25,2.54,3.25,3.83,7.17,2.79,8.54,12.04,15.83,17.37,8.87,8.75,10.08,14.17,13.37,6.46,5.33,6.04,10.58,13.37,4.12,6.5,6.75,11.08,8.33,5.91,5.04,3.42,1.46,1.63,2.42,2.25,3.13,5.21,1.75,9.25,11.96,2.0,2.42,7.21,15.41,8.92,8.33,14.5,9.29,7.83,4.79,13.67,15.92,7.87,19.95,18.29,9.42,12.12,9.0,7.08,3.21,6.34,12.62,10.79,15.87,17.5,14.58,12.87,12.46,7.41,8.71,11.04,8.21,9.33,9.17,5.79,4.58,10.79,10.34,8.08,7.79,7.41,6.96,12.17,4.67,2.58,2.62,4.5,2.25,5.37,3.04,1.29,4.46,13.13,8.29,3.17,4.96,7.5,7.96,10.37,20.75,10.41,1.21,5.88,13.5,10.88,6.34,1.42,1.75,1.21,0.37,1.25,1.0,4.67,9.42,14.04,11.38,11.29,7.17,12.92,10.34,5.17,5.71,11.63,22.37,15.71,8.54,5.54,5.58,5.04,5.96,9.75,19.08,0.54,4.0,4.08,3.17,6.54,4.54,1.17,4.5,10.75,3.13,4.67,13.17,12.08,17.29,20.62,11.75,18.75,14.88,11.63,8.25,4.42,4.17,3.25,3.04,6.13,8.12,6.08,6.13,4.12,5.88,19.62,18.58,16.33,10.96,6.87,12.25,15.87,12.21,8.54,10.17,9.33,13.17,12.46,16.25,11.46,13.33,23.75,12.92,13.5,7.33,9.71,14.42,15.75,10.96,11.92,15.09,8.75,8.25,6.04,10.75,4.92,8.96,9.5,7.92,10.21,5.79,12.0,6.79,11.21,7.21,9.54,9.96,2.21,7.83,9.33,14.75,6.96,12.5,13.75,17.33,7.54,3.46,1.79,6.58,9.21,8.17,6.79,2.42,5.04,10.96,7.62,9.83,12.75,6.13,5.83,5.96,10.13,9.42,8.42,13.33,11.58,7.21,17.88,13.79,4.83,4.63,5.83,2.5,7.12,17.96,11.63,19.21,14.62,20.3,17.96,11.04,3.88,6.87,4.17,2.0,4.33,12.42,14.46,15.96,12.62,5.58,10.0,13.0,21.17,11.71,6.13,5.83,8.38,7.08,7.04,13.79,8.21,11.67,10.92,3.5,4.96,2.83,7.08,6.79,8.67,5.71,5.0,21.71,19.62,18.5,9.54,11.96,13.21,11.71,5.91,17.04,12.08,6.25,6.58,3.17,2.33,2.54,2.62,3.04,6.71,7.87,6.38,8.25,8.58,6.87,3.42,2.25,6.71,7.0,8.92,13.29,4.71,3.25,4.33,8.21,11.21,23.25,24.25,19.08,12.12,5.96,12.21,15.92,14.83,10.92,5.79,9.96,3.21,2.04,3.42,9.29,14.79,12.96,8.25,1.46,1.42,2.92,6.63,5.17,4.33,1.42,5.79,5.63,2.13,2.29,12.0,13.75,15.83,6.38,3.88,7.5,1.5,11.25,12.62,8.46,11.21,3.83,2.37,2.75,2.83,1.42,3.5,4.33,5.41,7.21,6.54,8.63,7.87,11.46,10.04,1.63,5.17,7.17,4.42,11.12,9.46,7.38,4.25,1.54,1.21,3.46,3.0,4.67,2.54,7.04,7.46,10.63,8.5,6.17,4.88,4.04,6.42,6.13,8.63,9.96,7.75,4.71,4.0,4.38,10.13,8.29,9.54,4.5,3.88,9.67,8.04,9.75,6.21,7.96,7.41,11.25,9.83,6.04,3.83,5.79,6.71,12.92,10.21,6.75,3.79,3.71,7.29,3.37,5.17,2.37,3.88,3.63,5.09,6.54,6.21,3.83,8.29,9.46,4.83,8.04,12.67,8.5,6.5,4.79,7.38,4.58,4.5,7.25,7.17,7.5,8.71,3.0,2.92,4.67,3.63,5.58,7.96,6.25,7.67,9.59,6.92,5.37,7.67,7.67,12.25,8.17,7.58,6.17,6.29,5.79,13.08,5.63,4.5,4.21,9.42,5.96,7.08,4.71,5.41,3.5,1.46,2.79,6.46,11.29,10.79,11.04,11.58,14.67,13.25,12.04,2.83,4.63,5.04,6.83,6.71,5.46,6.08,8.96,8.21,1.67,2.54,4.25,3.92,3.67,7.33,1.58,6.96,5.13,2.83,2.0,6.0,6.17,6.54,8.71,8.38,4.17,10.79,7.41,5.71,4.79,2.92,5.17,7.04,10.79,8.67,9.75,9.92,16.92,8.33,7.33,12.46,6.38,11.92,6.42,17.0,16.62,17.08,14.21,16.79,14.46,6.71,8.04,7.12,10.46,10.67,13.75,7.92,10.71,6.21,9.79,14.96,9.0,15.12,11.96,9.54,13.92,15.12,10.21,6.21,2.88,9.08,10.0,7.58,14.37,14.37,12.17,11.63,11.34,13.13,20.33,12.38,12.5,7.08,12.42,9.21,9.62,9.04,7.83,9.08,8.54,5.13,7.41,9.67,4.96,5.96,8.33,8.42,11.12,11.58,12.29,10.88,12.08,17.04,15.37,12.29,9.17,14.62,19.33,21.92,15.41,3.96,13.13,16.42,11.79,11.04,13.13,13.25,14.58,16.21,21.04,13.0,6.04,3.88,2.62,2.75,7.62,16.46,17.0,5.25,14.04,11.34,11.87,4.12,16.08,15.34,16.58,12.38,12.33,11.25,10.41,10.41,13.7,13.0,14.0,9.13,12.87,18.05,14.04,14.92,6.92,12.5,17.29,25.04,9.08,6.04,10.46,1.92,3.33,8.38,7.83,10.21,11.46,13.67,12.58,9.21,13.96,17.12,14.17,12.96,4.33,4.12,4.58,7.5,14.12,14.42,7.5,8.29,11.83,8.04,8.58,7.38,10.88,18.12,11.42,8.87,18.41,15.83,7.87,4.83,11.17,18.96,16.5,11.46,7.75,3.29,2.29,6.5,6.79,3.71,4.12,3.63,6.87,3.88,6.04,9.46,15.63,22.92,16.08,9.54,9.33,8.54,17.04,17.33,14.42,14.21,19.29,12.62,11.67,14.88,11.54,18.16,18.12,9.17,5.46,13.92,9.5,6.25,5.46,7.75,6.34,6.29,6.71,12.42,8.71,11.34,5.21,8.0,9.17,13.0,15.04,11.5,6.67,4.5,9.62,9.83,9.71,3.17,8.17,15.09,10.0,8.67,7.67,8.87,10.88,10.41,11.67,4.21,4.71,3.21,8.87,10.04,9.5,8.08,12.25,13.33,19.75,24.5,18.0,15.0,20.08,19.55,7.96,3.25,10.63,11.38,4.58,5.96,6.92,9.04,9.04,7.5,9.0,6.0,4.71,9.08,3.08,6.92,3.54,15.04,13.0,11.63,10.83,18.38,15.29,12.58,11.17,8.21,5.91,8.71,7.87,5.09,9.79,12.71,9.04,8.75,7.62,8.42,5.0,10.92,4.17,9.92,15.25,8.04,7.29,3.79,2.5,3.54,4.33,6.58,9.96,4.08,3.67,3.08,3.21,4.29,5.46,4.21,4.04,2.33,3.75,6.17,5.09,8.33,12.42,11.5,8.63,17.54,11.21,9.83,12.62,14.12,9.87,7.17,6.92,9.92,3.92,7.04,6.67,11.54,15.12,8.87,8.58,8.25,4.29,2.62,7.92,5.0,5.88,6.71,6.13,6.83,6.92,4.88,5.33,3.67,4.75,6.38,5.91,3.96,5.96,10.0,6.17,10.67,11.58,12.38,10.67,10.13,5.0,4.54,7.58,3.88,4.5,5.83,6.17,7.08,4.38,7.87,10.29,6.63,3.54,3.5,2.54,8.33,11.5,11.42,9.13,3.5,4.04,7.04,5.75,5.5,10.08,7.71,9.38,11.25,5.75,5.91,8.87,10.29,6.75,10.5,11.34,5.83,9.08,10.88,20.62,14.0,9.33,9.17,12.38,8.33,11.96,12.87,6.42,4.58,9.25,8.12,8.5,3.79,3.88,8.46,6.5,7.67,18.29,15.54,15.83,12.33,10.17,7.67,8.63,6.71,9.25,10.88,8.29,14.5,18.91,9.96,10.83,6.71,11.75,7.87,15.96,15.0,16.96,14.62,18.29,5.37,13.25,19.38,21.71,12.87,15.29,6.87,13.37,17.5,15.92,16.92,11.46,19.08,14.54,13.88,17.71,15.71,5.29,2.13,2.08,2.62,8.96,9.71,21.46,11.08,18.41,14.33,5.88,7.83,10.67,13.54,13.25,5.33,4.5,2.71,5.79,5.63,2.67,3.13,8.29,11.58,17.41,10.0,12.96,22.04,7.92,4.5,12.17,25.12,11.12,14.71,11.29,12.87,9.54,12.83,16.71,6.83,13.08,9.59,14.71,11.08,8.0,4.25,6.34,9.67,13.42,20.08,12.71,8.79,14.09,14.71,18.5,24.21,16.88,10.88,7.92,3.88,2.79,9.08,12.0,11.58,13.13,17.83,16.38,9.13,5.46,3.17,6.96,4.58,13.54,14.83,7.46,8.71,7.71,13.13,6.58,5.21,6.46,3.79,3.04,6.58,3.5,4.54,2.92,3.17,4.92,6.96,3.67,3.83,7.12,9.0,8.38,16.38,13.75,8.21,11.25,9.79,12.17,14.12,18.88,12.42,17.41,12.21,21.54,17.0,18.08,11.46,12.04,8.0,9.0,10.04,10.75,8.08,8.54,6.63,8.46,8.71,12.96,9.08,12.96,11.71,14.37,20.17,15.54,5.04,3.79,1.75,5.58,6.63,8.12,10.58,7.62,11.75,12.17,10.92,9.0,12.38,12.87,14.5,17.16,11.12,14.0,7.96,3.29,4.08,9.13,14.29,9.29,13.04,12.92,6.54,8.54,4.79,5.46,4.75,5.09,2.25,4.83,2.96,9.21,8.0,8.12,8.58,6.87,3.17,3.71,3.21,1.04,6.63,14.79,10.0,9.67,13.75,11.04,5.37,8.17,9.04,11.21,7.79,10.0,10.88,3.04,2.75,4.5,2.71,2.54,3.63,2.67,5.96,13.21,12.79,10.79,6.0,9.38,3.58,5.91,6.42,4.33,4.08,5.5,10.0,10.92,5.54,5.63,5.09,11.96,17.92,7.54,6.58,12.12,10.5,8.38,8.33,7.0,1.42,4.79,6.29,4.71,2.37,2.25,2.08,3.08,5.09,5.41,7.79,11.83,8.38,11.79,12.21,7.54,15.63,3.83,2.83,3.21,3.42,2.42,2.54,4.21,5.83,3.17,3.96,7.17,7.71,5.91,2.25,1.38,0.71,3.21,2.92,3.67,5.75,10.41,15.41,11.96,5.04,3.71,6.13,2.83,5.63,4.92,1.83,2.04,6.79,4.88,4.04,10.17,5.0,6.79,6.04,8.42,6.25,6.29,2.96,9.17,11.71,6.25,4.12,5.66,8.12,14.75,16.42,13.5,10.17,6.21,3.58,7.17,3.88,6.38,4.25,3.0,7.17,5.09,6.96,9.92,12.62,9.42,2.58,10.25,10.21,7.96,10.46,13.37,12.17,6.54,7.12,3.88,4.5,1.79,3.58,3.79,3.21,1.58,2.62,8.87,5.17,4.54,5.54,6.5,10.41,6.08,10.46,16.66,13.67,8.83,9.0,20.75,12.87,7.38,7.87,6.21,2.75,10.04,4.33,0.63,0.0,3.29,8.38,13.04,4.67,7.62,4.46,7.5,2.33,4.79,9.13,7.46,14.21,14.09,9.62,10.34,11.92,12.33,8.96,15.92,10.88,18.88,19.25,14.29,14.33,16.79,11.96,7.41,18.25,2.92,4.88,3.21,3.04,3.92,11.17,11.96,10.96,9.13,16.33,8.71,3.83,9.54,15.34,15.0,14.04,12.62,9.04,11.46,13.33,7.29,16.92,20.25,20.83,13.92,13.67,17.16,16.96,19.0,20.62,15.46,7.25,5.75,8.71,15.54,12.04,13.83,15.46,15.96,20.41,15.46,10.0,14.62,15.92,15.09,10.29,9.5,8.25,12.21,6.83,13.62,14.17,19.79,11.38,8.92,7.71,11.96,11.83,15.92,7.08,3.58,3.37,10.88,11.96,12.71,7.0,2.13,4.63,2.29,3.79,12.12,12.04,7.83,9.83,9.33,3.83,12.54,9.17,9.13,11.46,12.79,11.38,5.25,4.04,7.38,14.04,18.96,15.75,11.29,8.12,4.92,4.38,6.71,13.7,15.0,4.83,2.88,2.42,4.33,2.33,2.21,6.25,8.38,9.17,5.29,4.75,4.25,6.87,5.17,5.09,3.92,3.37,8.63,11.92,8.0,6.75,1.83,2.96,2.75,2.25,2.96,1.33,3.29,2.21,3.33,3.29,4.79,7.62,9.5,11.75,4.33,7.41,13.7,8.71,14.54,12.92,10.29,5.91,6.58,13.0,18.58,13.54,5.58,17.5,12.92,9.13,10.79,7.54,8.92,2.67,5.91,9.96,8.58,14.62,14.92,11.38,1.58,2.37,6.17,12.58,6.46,6.71,6.04,9.33,12.04,9.21,5.63,11.75,17.5,10.21,7.21,10.34,11.38,14.12,3.42,5.33,4.25,3.29,2.37,6.17,7.46,5.75,7.38,6.08,11.54,8.5,6.87,8.42,11.38,10.29,4.17,3.17,5.25,12.96,7.58,11.12,8.21,12.42,7.41,7.0,7.96,10.41,11.54,10.67,8.5,5.91,7.62,12.5,11.17,7.12,7.5,13.17,11.71,16.29,17.29,10.08,8.58,7.87,6.79,10.63,11.29,8.5,6.0,3.13,5.96,6.83,2.5,3.79,4.17,5.29,7.12,5.71,7.62,11.38,7.0,10.63,4.79,10.25,14.25,10.96,5.88,5.46,4.54,8.87,7.83,4.92,9.59,10.04,10.54,6.67,8.5,7.71,7.46,7.46,3.33,4.42,12.92,10.54,12.79,8.04,8.29,16.21,10.88,6.92,8.0,9.13,9.42,10.37,5.46,2.79,6.13,7.67,4.58,9.5,9.67,13.37,12.21,11.17,10.25,13.0,8.04,6.46,8.38,7.83,7.46,6.79,13.92,9.92,10.17,9.62,13.29,12.67,9.33,8.38,10.0,9.75,8.33,8.17,4.0,3.37,5.41,7.87,14.37,11.71,15.59,11.87,10.96,8.87,10.79,13.67,15.59,22.29,15.92,13.13,11.42,8.54,6.92,9.21,12.71,5.96,6.79,5.41,5.33,11.21,20.04,19.25,20.21,11.75,12.79,19.08,10.63,9.62,5.88,8.25,4.71,14.29,9.25,4.83,9.71,18.0,17.79,6.96,22.5,20.67,9.96,13.29,14.62,9.87,12.04,17.41,15.75,17.04,21.67,14.92,17.46,20.21,13.75,17.58,13.13,14.88,10.58,20.79,25.46,16.66,18.63,17.46,16.25,15.21,17.75,11.12,17.83,19.5,20.5,25.33,14.96,11.08,15.25,8.12,8.58,9.96,16.38,21.59,19.12,10.88,10.63,14.25,11.83,17.41,15.79,19.62,13.96,10.83,5.88,9.62,7.25,12.12,16.79,14.42,20.91,19.5,10.92,13.17,10.25,6.67,14.75,7.87,14.92,14.58,9.62,4.42,6.08,4.12,8.12,8.46,8.58,7.54,5.21,7.0,5.66,4.67,9.79,4.21,6.96,12.38,14.0,4.0,10.63,6.0,5.21,7.92,6.96,6.34,9.21,9.25,7.87,7.92,10.13,10.0,7.38,1.63,4.88,14.46,14.0,5.33,10.08,12.96,19.29,10.75,7.62,6.46,7.83,5.58,7.79,10.5,9.62,8.71,11.12,10.92,10.63,11.67,10.88,7.33,10.17,9.42,9.62,8.58,9.46,11.71,10.75,11.79,9.79,7.0,8.33,17.71,15.92,10.54,11.79,12.83,10.29,9.08,10.0,4.46,5.54,8.38,6.79,9.79,11.38,4.67,4.04,2.54,1.21,1.87,3.08,4.83,7.21,12.96,10.41,13.88,4.38,2.54,0.92,5.63,6.96,12.04,11.08,7.41,9.54,5.33,7.33,2.13,6.21,7.87,3.13,2.21,3.92,4.04,2.54,3.71,6.21,5.88,5.71,7.96,9.62,9.92,6.87,3.92,5.33,3.0,8.38,17.21,5.91,3.75,9.13,7.46,2.88,3.92,4.58,1.58,1.58,3.75,7.58,9.0,5.63,3.33,4.42,10.46,11.71,4.42,3.54,2.5,6.04,4.08,1.79,5.46,6.87,6.04,1.83,1.63,2.92,4.79,3.96,4.12,4.71,4.63,8.5,8.67,8.29,6.67,9.13,5.96,6.58,10.71,14.37,7.58,2.79,5.21,8.54,9.79,13.79,17.29,14.62,13.75,9.59,9.46,4.08,5.83,6.04,3.75,6.04,2.71,3.33,4.38,3.83,7.75,9.46,4.83,4.83,5.29,0.92,2.75,5.88,2.62,3.25,5.75,9.54,2.79,5.33,8.38,11.5,8.42,9.42,6.29,6.34,5.0,2.46,3.0,3.17,8.29,14.0,6.42,5.5,7.46,6.54,9.71,7.04,5.09,4.96,10.0,11.5,9.33,8.79,12.87,9.25,12.92,7.75,4.92,6.29,6.63,8.25,11.42,9.29,16.25,12.25,15.67,12.42,8.12,14.12,9.96,14.42,8.67,11.25,9.75,13.88,18.46,13.33,5.09,2.08,1.08,4.58,8.38,6.0,1.46,7.96,7.08,6.96,5.09,6.04,4.29,14.0,16.04,16.66,21.42,11.75,8.04,9.96,3.46,2.29,5.5,9.46,11.42,12.46,9.92,9.71,11.67,12.5,6.38,8.58,5.5,8.79,11.67,6.13,5.66,9.71,6.96,4.96,9.87,18.21,14.0,11.54,18.54,7.08,0.63,9.38,9.21,10.92,11.87,12.0,18.66,10.83,8.38,10.63,13.96,16.62,10.46,11.29,10.46,6.29,8.87,6.04,7.29,5.71,5.33,11.63,9.5,8.42,7.92,9.42,12.04,10.0,6.67,9.17,12.33,10.88,12.5,12.21,13.96,11.38,13.17,13.04,9.46,17.79,7.08,13.17,18.96,13.29,13.62,15.63,14.83,14.58,14.37,7.17,21.29,19.12,16.42,11.46,11.92,8.38,8.25,3.42,13.83,20.75,25.96,20.79,21.67,14.12,13.29,10.21,11.46,9.25,16.71,23.13,23.5,18.21,11.58,11.21,10.58,13.62,12.79,4.58,9.5,11.17,16.17,15.16,17.12,17.41,8.79,4.25,9.42,8.5,4.38,2.92,10.08,12.25,10.17,12.08,11.29,13.29,14.17,8.46,5.75,6.71,11.71,6.5,6.96,10.04,7.87,11.54,20.79,14.33,7.54,7.58,12.71,11.34,15.92,12.0,15.37,9.59,10.79,8.04,11.12,14.58,16.88,6.83,5.71,5.96,12.96,15.29,16.58,20.88,18.71,17.0,15.63,17.0,6.87,10.79,12.87,12.21,16.38,14.71,10.63,7.21,6.0,10.34,11.34,8.29,15.0,17.67,8.79,6.46,9.42,5.63,5.88,8.0,8.83,14.79,9.96,9.87,11.25,8.25,9.59,7.29,3.54,5.54,11.96,12.38,14.96,7.08,5.54,5.66,5.33,7.25,4.33,11.25,14.17,16.29,8.96,13.37,13.25,16.5,13.88,7.04,7.08,8.38,9.83,10.54,9.5,11.08,10.0,7.12,3.29,4.63,3.33,8.29,6.29,12.92,4.29,3.04,5.75,6.5,7.87,5.54,7.75,10.83,10.83,10.63,13.13,10.79,11.92,9.79,3.37,7.0,11.67,11.08,9.96,7.12,8.96,10.04,10.37,7.0,7.41,3.42,5.58,5.58,6.04,5.37,4.67,4.88,4.08,1.54,4.12,5.5,4.29,10.34,9.13,9.33,5.75,9.04,10.34,10.08,6.54,5.79,10.25,9.96,8.83,7.92,5.63,8.96,6.58,5.58,5.09,4.92,5.5,9.25,9.87,7.71,13.0,10.58,5.91,6.75,4.92,3.46,3.08,1.67,1.67,2.67,4.71,1.46,2.42,3.79,3.42,1.87,2.0,4.38,3.0,4.04,6.87,7.08,5.79,6.04,4.12,3.88,8.87,13.59,9.67,9.21,8.63,10.34,11.63,10.04,8.08,3.79,4.46,4.38,13.17,21.25,10.54,20.54,12.33,6.71,12.17,6.58,3.08,4.42,5.66,9.75,7.25,3.21,1.63,6.34,15.92,10.13,8.17,7.04,7.71,6.96,5.25,5.04,2.67,8.96,5.41,11.54,19.21,7.71,3.04,1.63,7.41,5.29,8.33,2.25,13.83,14.83,4.88,13.5,1.54,2.42,6.96,9.17,6.04,6.21,1.92,0.92,2.58,6.54,2.58,6.71,4.46,4.21,10.0,11.79,7.54,5.96,9.62,5.17,8.08,5.33,5.75,8.38,4.96,8.83,6.58,11.38,8.29,8.63,7.41,7.5,2.17,3.71,8.46,7.83,8.54,11.21,13.04,16.75,15.5,21.84,13.21,8.29,10.88,10.0,5.75,11.0,11.83,16.75,15.46,10.75,7.25,9.0,2.71,1.21,1.21,5.33,5.66,6.63,5.63,9.92,3.17,6.21,6.5,9.71,6.63,8.25,5.96,6.5,11.67,7.58,12.33,9.67,7.92],"type":"scatter"},{"line":{"color":"orange"},"mode":"lines","name":"Validation","x":[5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000,6001,6002,6003,6004,6005,6006,6007,6008,6009,6010,6011,6012,6013,6014,6015,6016,6017,6018,6019,6020,6021,6022,6023,6024,6025,6026,6027,6028,6029,6030,6031,6032,6033,6034,6035,6036,6037,6038,6039,6040,6041,6042,6043,6044,6045,6046,6047,6048,6049,6050,6051,6052,6053,6054,6055,6056,6057,6058,6059,6060,6061,6062,6063,6064,6065,6066,6067,6068,6069,6070,6071,6072,6073,6074,6075,6076,6077,6078,6079,6080,6081,6082,6083,6084,6085,6086,6087,6088,6089,6090,6091,6092,6093,6094,6095,6096,6097,6098,6099,6100,6101,6102,6103,6104,6105,6106,6107,6108,6109,6110,6111,6112,6113,6114,6115,6116,6117,6118,6119,6120,6121,6122,6123,6124,6125,6126,6127,6128,6129,6130,6131,6132,6133,6134,6135,6136,6137,6138,6139,6140,6141,6142,6143,6144,6145,6146,6147,6148,6149,6150,6151,6152,6153,6154,6155,6156,6157,6158,6159,6160,6161,6162,6163,6164,6165,6166,6167,6168,6169,6170,6171,6172,6173,6174,6175,6176,6177,6178,6179,6180,6181,6182,6183,6184,6185,6186,6187,6188,6189,6190,6191,6192,6193,6194,6195,6196,6197,6198,6199,6200,6201,6202,6203,6204,6205,6206,6207,6208],"y":[10.75,9.46,7.87,15.41,11.5,8.04,8.04,11.38,12.71,10.75,7.62,4.96,17.04,22.08,11.75,7.17,9.21,14.83,7.17,17.12,14.79,4.63,2.04,10.34,10.88,4.96,12.17,8.58,2.54,12.79,4.75,6.17,9.21,18.29,10.5,10.08,7.58,16.66,7.96,9.96,12.25,9.13,5.25,5.46,8.21,11.5,5.91,11.04,15.09,13.04,9.08,3.83,10.96,17.92,17.71,10.46,8.71,17.92,16.71,5.0,13.25,16.71,18.66,11.04,9.42,14.0,17.67,16.88,20.21,14.29,9.67,13.33,16.33,19.79,15.34,18.34,15.41,11.08,17.41,18.21,13.5,12.08,9.71,12.08,12.54,19.33,10.13,7.79,14.12,19.46,25.62,12.33,14.92,9.38,11.17,15.67,13.17,8.63,6.13,13.83,13.04,21.62,18.54,14.12,8.0,8.42,8.58,8.21,7.75,14.29,18.46,26.42,20.25,13.21,14.5,14.37,17.54,14.09,13.79,10.58,6.75,7.41,11.04,8.96,7.96,9.87,17.08,8.87,6.58,7.67,15.71,10.75,7.96,6.67,8.71,9.62,5.09,4.71,4.5,10.83,11.96,8.08,9.04,11.58,11.5,10.54,9.04,7.29,10.17,12.5,7.58,5.29,6.87,8.5,12.83,13.04,14.67,11.67,6.71,7.58,16.0,14.79,7.67,6.21,11.08,11.0,10.0,9.42,5.54,4.0,4.0,2.96,2.79,5.0,6.04,8.79,9.75,12.25,7.08,7.5,14.96,12.29,13.08,7.33,4.17,4.21,6.21,4.29,8.42,8.87,8.42,6.79,9.04,6.79,6.17,7.17,9.08,14.17,18.16,17.5,15.29,7.83,13.21,15.5,17.21,15.41,11.58,9.42,9.21,7.38,7.41,7.21,4.96,8.87,9.62,14.88,9.0,6.34,7.04,4.46,3.71,5.83,6.5,7.21,6.54,7.87,7.67,15.37,19.17,9.17,7.12,7.46,7.96,9.21,8.54,16.29,9.67,10.67,10.54,9.08,11.58,7.46,7.46,15.09,14.37,12.12,16.04,11.63,14.46,8.17,10.58,11.75,20.46,14.04,7.92,4.58,8.58,2.42,13.46,10.58,8.38,8.5,7.5,5.13,5.0,15.21,20.12,11.08,8.67,9.83,21.09,20.3,21.12,18.5,10.79,10.17,11.21,7.75,14.96,10.88,10.41,15.25,9.75,16.29,16.71,10.58,12.54,18.21,11.29,13.33,12.17,12.04,8.92,11.83,10.58,16.5,14.88,9.79,13.0,13.25,14.92,14.37,18.05,11.75,11.12,8.46,18.29,12.62,15.29,17.46,19.33,16.88,12.12,17.79,24.13,19.29,16.21,28.16,18.79,13.54,13.59,11.38,19.92,13.33,14.17,13.46,23.83,17.08,7.54,4.04,4.0,5.71,1.87,2.13,8.96,20.33,24.08,22.46,20.21,18.34,15.75,9.0,14.17,9.75,12.33,12.33,13.42,13.79,7.58,6.54,2.67,2.71,4.79,11.67,14.17,10.71,12.96,21.75,10.96,11.96,16.08,12.58,15.71,16.83,15.59],"type":"scatter"},{"line":{"color":"green"},"mode":"lines","name":"Test","x":[6209,6210,6211,6212,6213,6214,6215,6216,6217,6218,6219,6220,6221,6222,6223,6224,6225,6226,6227,6228,6229,6230,6231,6232,6233,6234,6235,6236,6237,6238,6239,6240,6241,6242,6243,6244,6245,6246,6247,6248,6249,6250,6251,6252,6253,6254,6255,6256,6257,6258,6259,6260,6261,6262,6263,6264,6265,6266,6267,6268,6269,6270,6271,6272,6273,6274,6275,6276,6277,6278,6279,6280,6281,6282,6283,6284,6285,6286,6287,6288,6289,6290,6291,6292,6293,6294,6295,6296,6297,6298,6299,6300,6301,6302,6303,6304,6305,6306,6307,6308,6309,6310,6311,6312,6313,6314,6315,6316,6317,6318,6319,6320,6321,6322,6323,6324,6325,6326,6327,6328,6329,6330,6331,6332,6333,6334,6335,6336,6337,6338,6339,6340,6341,6342,6343,6344,6345,6346,6347,6348,6349,6350,6351,6352,6353,6354,6355,6356,6357,6358,6359,6360,6361,6362,6363,6364,6365,6366,6367,6368,6369,6370,6371,6372,6373,6374,6375,6376,6377,6378,6379,6380,6381,6382,6383,6384,6385,6386,6387,6388,6389,6390,6391,6392,6393,6394,6395,6396,6397,6398,6399,6400,6401,6402,6403,6404,6405,6406,6407,6408,6409,6410,6411,6412,6413,6414,6415,6416,6417,6418,6419,6420,6421,6422,6423,6424,6425,6426,6427,6428,6429,6430,6431,6432,6433,6434,6435,6436,6437,6438,6439,6440,6441,6442,6443,6444,6445,6446,6447,6448,6449,6450,6451,6452,6453,6454,6455,6456,6457,6458,6459,6460,6461,6462,6463,6464,6465,6466,6467,6468,6469,6470,6471,6472,6473,6474,6475,6476,6477,6478,6479,6480,6481,6482,6483,6484,6485,6486,6487,6488,6489,6490,6491,6492,6493,6494,6495,6496,6497,6498,6499,6500,6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516,6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532,6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548,6549,6550,6551,6552,6553,6554,6555,6556,6557,6558,6559,6560,6561,6562,6563,6564,6565,6566,6567,6568,6569,6570,6571,6572,6573],"y":[14.71,17.08,21.96,8.38,13.88,11.29,8.08,8.12,17.33,16.62,19.41,11.58,9.87,4.17,1.58,7.87,8.33,7.92,10.29,13.79,11.08,15.71,13.5,13.92,8.33,16.5,11.63,15.87,25.54,6.08,8.0,20.96,14.54,8.12,8.75,13.46,9.59,5.5,9.38,15.71,6.79,5.09,9.42,9.08,5.75,11.87,14.46,13.33,23.42,27.71,16.75,10.08,15.37,13.0,14.92,9.87,3.54,6.29,10.54,12.29,6.38,2.29,4.67,6.08,8.58,14.17,10.67,2.79,4.63,3.63,13.13,11.79,21.87,16.08,9.13,6.5,7.83,17.0,13.13,14.29,18.0,20.12,15.46,17.25,15.92,20.17,18.08,20.08,11.46,12.54,2.67,4.71,6.71,9.83,9.5,7.58,6.5,2.04,8.33,11.79,9.5,8.58,14.25,9.46,4.08,5.83,15.16,8.12,11.12,11.25,7.38,9.5,8.42,9.5,13.0,8.54,6.5,3.33,7.25,17.16,11.17,9.62,3.96,5.96,8.79,5.41,3.67,6.34,4.79,5.04,10.88,13.79,10.08,4.58,2.37,3.33,3.88,3.17,3.04,2.29,3.04,2.92,8.5,8.29,3.92,0.83,1.67,2.13,4.88,4.88,4.83,6.46,2.79,4.54,5.63,11.75,9.29,7.12,6.34,9.5,8.71,8.04,6.58,5.04,4.75,4.08,16.5,12.42,1.71,7.41,4.54,3.04,6.21,10.34,12.67,9.33,9.42,10.79,14.0,9.67,9.17,12.42,11.92,13.7,13.7,12.38,11.79,11.92,11.67,4.42,3.88,2.83,1.63,0.83,3.17,2.79,2.33,1.83,7.17,11.87,7.58,4.67,13.83,10.08,9.33,8.42,7.04,10.5,9.92,4.08,2.92,12.17,9.33,4.5,9.62,5.54,1.38,6.92,5.96,7.75,3.17,3.08,6.42,5.54,6.25,7.5,8.54,10.83,4.79,9.92,3.88,5.41,12.38,4.33,1.25,1.33,1.46,0.75,1.54,2.42,6.92,7.58,8.87,7.25,6.13,4.12,5.09,3.25,7.38,9.21,8.21,15.29,15.67,18.5,9.08,12.62,13.67,14.96,14.04,8.25,5.71,1.21,3.04,5.5,9.75,11.25,12.87,12.75,16.58,13.79,19.41,22.21,16.92,4.21,6.83,9.25,11.29,15.34,8.58,9.04,7.62,7.62,6.34,11.29,2.13,3.0,1.25,7.41,15.83,12.62,4.58,5.54,9.21,9.67,8.29,11.0,17.67,16.17,8.58,1.83,2.42,3.42,8.92,4.63,8.17,10.0,13.21,14.58,8.92,12.21,19.29,8.87,6.21,5.91,4.12,14.96,16.5,17.54,20.12,17.67,17.58,12.08,11.08,16.08,14.67,16.71,12.25,7.17,14.62,8.08,2.33,1.96,4.38,7.5,21.54,12.33,17.46,8.38,13.92,24.54,19.92,10.34,10.75,13.21,14.83,10.54,13.0,8.83,12.67,9.13,2.46,2.42,4.75,4.54,11.67,2.46,16.5,12.25,8.71,1.96,14.46,14.33,19.17,18.08,19.25],"type":"scatter"},{"line":{"color":"red","dash":"dash"},"mode":"lines","name":"Prediction","x":[6209,6210,6211,6212,6213,6214,6215,6216,6217,6218,6219,6220,6221,6222,6223,6224,6225,6226,6227,6228,6229,6230,6231,6232,6233,6234,6235,6236,6237,6238,6239,6240,6241,6242,6243,6244,6245,6246,6247,6248,6249,6250,6251,6252,6253,6254,6255,6256,6257,6258,6259,6260,6261,6262,6263,6264,6265,6266,6267,6268,6269,6270,6271,6272,6273,6274,6275,6276,6277,6278,6279,6280,6281,6282,6283,6284,6285,6286,6287,6288,6289,6290,6291,6292,6293,6294,6295,6296,6297,6298,6299,6300,6301,6302,6303,6304,6305,6306,6307,6308,6309,6310,6311,6312,6313,6314,6315,6316,6317,6318,6319,6320,6321,6322,6323,6324,6325,6326,6327,6328,6329,6330,6331,6332,6333,6334,6335,6336,6337,6338,6339,6340,6341,6342,6343,6344,6345,6346,6347,6348,6349,6350,6351,6352,6353,6354,6355,6356,6357,6358,6359,6360,6361,6362,6363,6364,6365,6366,6367,6368,6369,6370,6371,6372,6373,6374,6375,6376,6377,6378,6379,6380,6381,6382,6383,6384,6385,6386,6387,6388,6389,6390,6391,6392,6393,6394,6395,6396,6397,6398,6399,6400,6401,6402,6403,6404,6405,6406,6407,6408,6409,6410,6411,6412,6413,6414,6415,6416,6417,6418,6419,6420,6421,6422,6423,6424,6425,6426,6427,6428,6429,6430,6431,6432,6433,6434,6435,6436,6437,6438,6439,6440,6441,6442,6443,6444,6445,6446,6447,6448,6449,6450,6451,6452,6453,6454,6455,6456,6457,6458,6459,6460,6461,6462,6463,6464,6465,6466,6467,6468,6469,6470,6471,6472,6473,6474,6475,6476,6477,6478,6479,6480,6481,6482,6483,6484,6485,6486,6487,6488,6489,6490,6491,6492,6493,6494,6495,6496,6497,6498,6499,6500,6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516,6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532,6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548,6549,6550,6551,6552,6553,6554,6555,6556,6557,6558,6559,6560,6561,6562,6563,6564,6565,6566,6567,6568,6569,6570,6571,6572,6573],"y":[14.71,17.08,21.96,10.365074398667652,10.91757606221858,11.046475642608037,8.482239748931658,8.151708362030003,16.947610576047964,16.729008217927024,19.28403218141616,12.707976938789773,10.023679928175588,4.2122712405151255,1.5864466947907763,6.4577367755459045,8.148790752204674,8.054529187800727,10.193413290450897,13.771617071345839,11.623206426877156,14.817658396648762,13.321010652557858,14.066310565752786,8.385009353615128,14.445624107265331,13.586285606032991,15.00411974833479,25.309974769571138,10.042324279428307,7.767806379226716,18.816959043630508,16.290543141417768,9.008978940354112,8.771642218650664,12.61178946518157,10.992830441418665,5.828215843128247,7.953404339013693,15.272489422780854,9.705850998133158,5.579683123285769,7.926216987208417,8.777815210999224,6.542798007303242,10.7831722034995,14.274597430422526,14.26620982385169,23.047929742918548,27.654976432211075,19.08145030779708,10.490914871534777,12.324985068164032,12.713583732830823,14.769770505221324,10.787087347906242,3.71664534299784,7.310693262607204,9.96023155797267,12.208163150275244,7.6853324336488615,2.5268900220578243,6.014671983604588,5.6532656668647014,8.496274411165441,14.151403013966505,11.596891919364284,3.2562432049939907,3.5159905440530146,3.5186267211260924,12.965639831862314,11.371047091055527,21.423496677740303,16.435573585865995,9.795972189392234,6.591145421615898,8.866517990242839,16.582061795995568,13.89229796220149,14.765797898446346,17.260625800472646,19.984252337921017,15.79354467753357,16.74356228943204,16.12527506365421,19.78951733969213,19.1661753404549,19.631709934325684,11.876171495108514,12.141518422533874,2.939363275073572,3.0153504690913535,6.058483248584677,9.703102512023554,9.578282447921607,7.909188510844322,6.555720213625895,2.0535508350102725,6.641260139407444,11.485468284355193,10.048692519429641,8.983783622058372,13.225872220567886,11.08016237109674,4.452745321537511,7.030513235925409,14.57083351534059,11.361753978458772,11.226666909799567,11.237682591414544,7.423361104204622,8.546915957064556,8.391905687012628,9.443685839831145,12.988271639438484,9.836963672039044,6.7314803117657185,3.378311157945947,6.292526591686979,16.846479616528217,13.31535224061123,9.677054118635514,4.120341473393328,6.666789354916661,8.261630392888033,6.836437213177482,3.982010288813509,6.345830737862892,5.177155677707004,5.1155130670367175,10.780245135411876,13.775449082224586,11.145699353252205,4.87442404323356,2.41114689870517,2.475231171326065,3.621698628498132,3.5282357718696558,3.1357412069343207,2.312055099372798,2.736296362920011,2.8815847440723994,8.484795440028783,8.252109003546952,4.34891809806606,0.9017957937605281,0.9504638284355267,1.854541043680518,4.8158170769068445,4.878758163167912,4.855127255392624,6.440407367332566,3.3895181284038163,5.070807077766057,5.386354568582707,11.695569238280605,10.153292802856306,7.576129985998187,6.402059188739491,8.758341638263449,8.727025152883812,8.243909676871358,6.6251896487429836,5.048080928728695,4.750861554945405,4.080330171175963,16.180820456701507,13.103648041167919,2.6200961006047567,5.489550513716481,4.185592856200218,3.518827285829698,6.003078505679433,10.250826365536735,12.65587433571637,10.471134850355192,9.576238568168739,10.2640404280375,13.873402615800405,11.006048188436814,9.376146667087324,11.516701727520596,11.843197866933206,13.661764219325919,13.699278859114017,12.548519293186267,11.816261783892688,11.848585248426431,11.701822499952492,4.428530814143136,3.880467640047073,2.8302482677736176,1.6300553045662094,0.8300100091668955,2.7684034623935876,2.7864435675678405,2.488730739043342,1.8634485559715468,6.877784635981299,11.820653565003909,9.461747167279867,5.064485399321313,11.840615419907648,11.735670744119489,8.966310872643948,8.51820756286826,7.062013533338633,9.822689243134112,9.902996564284015,4.385167816135262,2.954557501750966,11.174452793426749,10.898339790155042,5.161903677253918,7.51263970862904,6.671778277037017,1.7038747059001602,5.163193188492246,5.768902933046422,7.674769273198144,4.027328778724181,3.1724818821315215,5.907773032492429,5.67572825489575,6.161816951655797,7.478952163104455,8.536338897624915,10.829161399836037,5.735668748643997,8.08041939162309,5.239172471840578,5.31497284743622,12.036604488888552,6.7283397412091785,1.6775612444611148,1.3519168856309849,1.4050494196246097,0.7917994188802566,1.2983245507374293,2.369993726163962,6.909025096874969,7.579046544881964,8.869629602631795,7.789223852288779,6.234832073098227,4.141678791495583,4.434433893223414,3.5554188707735124,6.90284879970387,9.134257438370359,8.520669631738805,14.933601530614665,15.643371723253729,18.486938174123623,10.300227195102048,10.604709192596165,13.083186115732168,14.853912853842012,14.665694516752074,8.448690326768883,5.737823152721912,1.2170178176010495,1.6936642607554246,5.129227578374341,9.678204262408764,11.239947759604659,12.867426610854542,12.800261204113642,16.534225538588217,13.591469551596969,18.461724992303097,22.037269367237382,16.804732245488395,4.553749561707583,7.9473283329243145,9.833183987750486,11.109995691940444,15.298987370116741,10.328378317786957,8.972465821994616,7.679434478000104,7.621938362946162,6.342988141357948,10.767558684670222,3.895423082979918,3.125146199285923,1.3107947876439563,6.520442344034705,15.661667489401118,12.045277850920986,5.347119458350685,5.497297634775579,8.425279930580926,9.563792924678308,8.8920266490583,10.554797568216237,17.573319497887756,17.52681533503489,9.147148913767085,1.925868096995665,2.2060999997365287,2.8908562161852234,8.781137456385423,6.556146865393201,7.1735220711371745,9.720171716800182,13.154126178357917,14.571583888258301,9.754067119697426,10.699558241011655,18.873424356821747,11.937852031814861,6.724679753506763,5.958711724149509,4.143021432572935,14.07365350699718,16.404199399439634,17.519733148256265,20.11362108725565,18.636049557074017,17.732750210695908,12.125495918249747,11.08449185063087,15.2586128261261,15.145913927772568,16.486655108867865,12.89751343115923,7.2915249524143535,12.392244241732454,10.1755110331734,2.8389852046926363,1.9983904462040079,3.5214835275327108,7.31645058637938,21.500440198708677,15.66979373313178,17.000331767269987,8.824455718231246,11.280720270548226,23.889151885046697,17.910398996272697,11.372344627414078,10.790977690260885,12.166489161397507,14.612286778723096,11.625554137982036,12.028757808674715,9.354163779658528,11.56830687596692,10.143720179270074,2.709016224150614,2.4279823086797743,4.089748031922157,4.470030907452407,11.639969481224476,5.003295509523957,13.401382027063448,13.152016572456894,9.513158684059238,2.125770814611417,11.475149081958131,13.98725800376737,19.058369107059296,18.945936401905783,19.0896415708974],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"legend":{"x":0,"y":1,"traceorder":"normal"},"title":{"text":"Modelo Emsamblado Adaptativo"},"xaxis":{"title":{"text":"Observaciones"}},"yaxis":{"title":{"text":"Valores"}},"plot_bgcolor":"rgba(0,0,0,0)","width":1000,"height":600},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ba2e5094-85f7-4772-bc04-84b9f0ae8069');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div></div></div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ANNs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Redes Neuronales Artificiales</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Elianna Gomez F
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>